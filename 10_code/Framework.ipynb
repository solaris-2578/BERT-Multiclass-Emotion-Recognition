{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WigneUJ7g2J"
   },
   "source": [
    "# **Experiment 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 802
    },
    "colab_type": "code",
    "id": "IBLGq3tsHaZK",
    "outputId": "215a7e86-83ad-4096-ee1c-1fe725187fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.3)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
      "Collecting emoji\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=95372066a9e41dcfc7c64c8841d9b00d80f958d74e035f8fa17b096cd568a087\n",
      "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-0.5.4\n",
      "Collecting keras-self-attention\n",
      "  Downloading https://files.pythonhosted.org/packages/44/3e/eb1a7c7545eede073ceda2f5d78442b6cad33b5b750d7f0742866907c34b/keras-self-attention-0.42.0.tar.gz\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.18.3)\n",
      "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.3.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
      "Building wheels for collected packages: keras-self-attention\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-self-attention: filename=keras_self_attention-0.42.0-cp36-none-any.whl size=17296 sha256=58445a39c0d6ac12dac8bc8fb656a8d645b122edf703dac3eb045204d401a02b\n",
      "  Stored in directory: /root/.cache/pip/wheels/7b/05/a0/99c0cf60d383f0494e10eca2b238ea98faca9a1fe03cac2894\n",
      "Successfully built keras-self-attention\n",
      "Installing collected packages: keras-self-attention\n",
      "Successfully installed keras-self-attention-0.42.0\n",
      "Collecting wordninja\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/15/abe4af50f4be92b60c25e43c1c64d08453b51e46c32981d80b3aebec0260/wordninja-2.0.0.tar.gz (541kB)\n",
      "\u001b[K     |████████████████████████████████| 542kB 4.8MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: wordninja\n",
      "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wordninja: filename=wordninja-2.0.0-cp36-none-any.whl size=541552 sha256=32c832e3b97742dafbbc98779cdfa1b815bdddccc8052a7f65560f50f85c175c\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/46/06/9b6d10ed02c85e93c3bb33ac50e2d368b2586248f192a2e22a\n",
      "Successfully built wordninja\n",
      "Installing collected packages: wordninja\n",
      "Successfully installed wordninja-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install emoji --upgrade\n",
    "!pip install keras-self-attention\n",
    "!pip install wordninja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UaDxi6vN8yil"
   },
   "source": [
    "# **Headers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hcKlqkNXGbut",
    "outputId": "21f95add-8804-41b6-ac44-0bd0515238b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##################Headers#########\n",
    "import pandas as pd\n",
    "import keras\n",
    "import emoji\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, Dropout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BnECRf1Y89QB"
   },
   "source": [
    "# **Loading the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7r5Wv9y0Gtcm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3_81hGVHZ41"
   },
   "outputs": [],
   "source": [
    "classes_list = [\"NOT\",\"HOF\"]\n",
    "df_train[\"class\"]=df_train['task_1'].apply(classes_list.index)\n",
    "df_test[\"class\"]=df_test['task_1'].apply(classes_list.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OV8812OU9GaD"
   },
   "source": [
    "# ***Preprocessing Functions***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cI4fxCud9moQ"
   },
   "source": [
    "Expanding Contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "klvUPYGNvcOW"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#########Contraction List#####\n",
    "cList = {\"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\"you're\": \"you are\",\"you've\": \"you have\"}\n",
    "#########Function for contraction#####\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys())) #regular expression object, with format of contractions like i'm, i've\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)] #find the match in the list\n",
    "    return c_re.sub(replace, text)  #replacing the short hands with expanded version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNIcTaiy91Kq"
   },
   "source": [
    "Lemmatization according to sentence structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "9Maa-nAu97qF",
    "outputId": "9d0853f2-f823-47f5-d88b-0dac05887aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xb0sMADs8Qi2"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "lemma_function = WordNetLemmatizer()\n",
    "def lemmatize(row):\n",
    "  tokens = word_tokenize(row)\n",
    "  new =  \"\"\n",
    "  for token, tag in pos_tag(tokens):\n",
    "    if token not in stopwords.words('english'):\n",
    "      lemma = lemma_function.lemmatize(token, tag_map[tag[0]])\n",
    "      new=new+lemma+\" \"\n",
    "  return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "I75gg3gwo8qh",
    "outputId": "cadc83f3-dd85-4eaf-fb36-b84b08a7b623"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OPfEpo8v9_lA"
   },
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "491detyu-lD2"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "df_train[\"text\"]=df_train[\"text\"].str.replace(\"@\",\"username\") #removing @\n",
    "df_train[\"text\"]=df_train[\"text\"].str.replace(\"'s\",\"\")\n",
    "df_train[\"text\"]=df_train[\"text\"].str.replace(\"?\",\" question \")\n",
    "df_train[\"text\"]=df_train[\"text\"].str.replace(\"!\",\" exclamation \")\n",
    "df_train[\"text\"]=df_train[\"text\"].str.replace(r'http\\S+', '')# removing links\n",
    "df_train[\"text\"]=df_train[\"text\"].apply(expandContractions) #expanding contractions\n",
    "df_train[\"text\"]=df_train[\"text\"].apply(emoji.demojize)#changing emojis to text\n",
    "df_train[\"text\"]=df_train[\"text\"].str.replace(\"#\",\"\").str.replace(\"_\",\" \").str.replace(\":\",\" \").str.lower()\n",
    "df_train[\"text\"]=df_train[\"text\"].apply(lemmatize) #lemmatization\n",
    "df_train[\"text\"]=df_train[\"text\"].str.replace(r'[^\\w\\s]',\" \") #removing punctuation\n",
    "df_test[\"text\"]=df_test[\"text\"].str.replace(\"@\",\"username\")\n",
    "df_test[\"text\"]=df_test[\"text\"].str.replace(\"'s\",\"\")\n",
    "df_test[\"text\"]=df_test[\"text\"].str.replace(\"?\",\" question \")\n",
    "df_test[\"text\"]=df_test[\"text\"].str.replace(\"!\",\" exclamation \")\n",
    "df_test[\"text\"]=df_test[\"text\"].apply(split_hashtag)\n",
    "df_test[\"text\"]=df_test[\"text\"].str.replace(r'http\\S+', '')\n",
    "df_test[\"text\"]=df_test[\"text\"].apply(expandContractions)\n",
    "df_test[\"text\"]=df_test[\"text\"].apply(emoji.demojize)\n",
    "df_test[\"text\"]=df_test[\"text\"].str.replace(\"#\",\"\").str.replace(\"_\",\" \").str.replace(\":\",\"\").str.lower()\n",
    "df_test[\"text\"]=df_test[\"text\"].apply(lemmatize)\n",
    "df_test[\"text\"]=df_test[\"text\"].str.replace(r'[^\\w\\s]',\"\")\n",
    "tokenizer = Tokenizer(num_words=5000) # tokenizing\n",
    "tokenizer.fit_on_texts(df_train[\"text\"].values.tolist())\n",
    "#Creating the training and testing sets consisting of tokens\n",
    "X_train = tokenizer.texts_to_sequences(df_train[\"text\"].values.tolist())\n",
    "X_test = tokenizer.texts_to_sequences(df_test[\"text\"].values.tolist())\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "word_index=tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQCfykSh_QRt"
   },
   "source": [
    "Preparing the training and testing sets for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YhzvBMmtKV4U"
   },
   "outputs": [],
   "source": [
    "maxlen=max([len(a) for a in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_OppxtnV0iI"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SyZP2TN8L0bI"
   },
   "outputs": [],
   "source": [
    "y_train=df_train[\"class\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhBYHZul-05p"
   },
   "source": [
    "# **Function for Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xEMjJ8TcSxpv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def make_glovevec(glovepath, max_features, embed_size, word_index, veclen=300):\n",
    "    embeddings_index = {}\n",
    "    f = open(glovepath)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        dimension=len(values)-1\n",
    "        word = ' '.join(values[:-300])\n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embeddings_index[word] = coefs.reshape(-1)\n",
    "    f.close()\n",
    "\n",
    "    nb_words = min(max_features, len(word_index)+1)\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fEKYaseJaLm9"
   },
   "outputs": [],
   "source": [
    "max_features=len(word_index)\n",
    "embed_size=300\n",
    "embedding_vector = make_glovevec(\"/content/drive/My Drive/english_dataset/glove.840B.300d.txt\", max_features+1, embed_size, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sO6upg0n_fIK"
   },
   "source": [
    "# The Classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q0UksjuIcR5F"
   },
   "outputs": [],
   "source": [
    " ############### Keras\n",
    "from keras.models import Sequential\n",
    "from keras_self_attention import SeqSelfAttention as Attention\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "PUz-lQpA-SSI",
    "outputId": "cf9c6542-0bd2-40ae-ae67-a10b1e832a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4681 samples, validate on 1171 samples\n",
      "Epoch 1/10\n",
      "4681/4681 [==============================] - 243s 52ms/step - loss: 0.5850 - accuracy: 0.7195 - val_loss: 0.8603 - val_accuracy: 0.3928\n",
      "Epoch 2/10\n",
      "4681/4681 [==============================] - 242s 52ms/step - loss: 0.5371 - accuracy: 0.7411 - val_loss: 1.0411 - val_accuracy: 0.3518\n",
      "Epoch 3/10\n",
      "4681/4681 [==============================] - 244s 52ms/step - loss: 0.5154 - accuracy: 0.7522 - val_loss: 0.9542 - val_accuracy: 0.4210\n",
      "Epoch 4/10\n",
      "4681/4681 [==============================] - 241s 52ms/step - loss: 0.4965 - accuracy: 0.7599 - val_loss: 1.0315 - val_accuracy: 0.4116\n",
      "Epoch 5/10\n",
      "4681/4681 [==============================] - 243s 52ms/step - loss: 0.4739 - accuracy: 0.7802 - val_loss: 1.2553 - val_accuracy: 0.3911\n",
      "Epoch 6/10\n",
      "4681/4681 [==============================] - 247s 53ms/step - loss: 0.4561 - accuracy: 0.7868 - val_loss: 1.0087 - val_accuracy: 0.3911\n",
      "Epoch 7/10\n",
      "4681/4681 [==============================] - 239s 51ms/step - loss: 0.4343 - accuracy: 0.8000 - val_loss: 1.0933 - val_accuracy: 0.3962\n",
      "Epoch 8/10\n",
      "4681/4681 [==============================] - 237s 51ms/step - loss: 0.3944 - accuracy: 0.8242 - val_loss: 1.3671 - val_accuracy: 0.3962\n",
      "Epoch 9/10\n",
      "4681/4681 [==============================] - 241s 52ms/step - loss: 0.3499 - accuracy: 0.8441 - val_loss: 1.6297 - val_accuracy: 0.3894\n",
      "Epoch 10/10\n",
      "4681/4681 [==============================] - 239s 51ms/step - loss: 0.2959 - accuracy: 0.8678 - val_loss: 1.2961 - val_accuracy: 0.4714\n",
      "Accuracy score = 0.8013876843018214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87       865\n",
      "           1       0.59      0.65      0.62       288\n",
      "\n",
      "    accuracy                           0.80      1153\n",
      "   macro avg       0.74      0.75      0.74      1153\n",
      "weighted avg       0.81      0.80      0.80      1153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################BiLSTM Attention+ Relu without any dropouts ######################\n",
    "d2=0.0\n",
    "d1=0.0\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(len(word_index)+1, 300, input_length=129, weights=[embedding_vector], trainable=False))\n",
    "model_glove.add(Bidirectional(LSTM(300, return_sequences=True, dropout=d1,recurrent_dropout=d2)))\n",
    "model_glove.add(Attention(attention_activation='sigmoid'))\n",
    "model_glove.add(Flatten())\n",
    "model_glove.add(Dense(10, activation='relu'))\n",
    "model_glove.add(Dropout(0.0)) \n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit train data\n",
    "model_glove.fit(X_train, y_train, validation_split=0.2, epochs = 10)\n",
    "y_pred = model_glove.predict_classes(X_test)\n",
    "y_test=df_test[\"class\"].values.tolist()\n",
    "print(\"Accuracy score =\", accuracy_score(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))   \n",
    "confusion_matrix(y_test, y_pred)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6R1ZlHqo73ej"
   },
   "source": [
    "# **Extras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "R2r8KRlyO3Zn",
    "outputId": "de65e2ac-1034-46de-b596-d20e954ad8f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4681 samples, validate on 1171 samples\n",
      "Epoch 1/10\n",
      "4681/4681 [==============================] - 254s 54ms/step - loss: 0.5797 - accuracy: 0.7090 - val_loss: 0.9242 - val_accuracy: 0.4142\n",
      "Epoch 2/10\n",
      "4681/4681 [==============================] - 243s 52ms/step - loss: 0.5352 - accuracy: 0.7419 - val_loss: 0.9968 - val_accuracy: 0.3749\n",
      "Epoch 3/10\n",
      "4681/4681 [==============================] - 248s 53ms/step - loss: 0.5158 - accuracy: 0.7601 - val_loss: 0.9396 - val_accuracy: 0.4031\n",
      "Epoch 4/10\n",
      "4681/4681 [==============================] - 243s 52ms/step - loss: 0.5003 - accuracy: 0.7631 - val_loss: 1.2389 - val_accuracy: 0.3723\n",
      "Epoch 5/10\n",
      "4681/4681 [==============================] - 239s 51ms/step - loss: 0.4740 - accuracy: 0.7746 - val_loss: 1.1495 - val_accuracy: 0.3749\n",
      "Epoch 6/10\n",
      "4681/4681 [==============================] - 223s 48ms/step - loss: 0.4344 - accuracy: 0.7934 - val_loss: 1.2238 - val_accuracy: 0.4082\n",
      "Epoch 7/10\n",
      "4681/4681 [==============================] - 222s 47ms/step - loss: 0.3946 - accuracy: 0.8212 - val_loss: 1.2246 - val_accuracy: 0.4065\n",
      "Epoch 8/10\n",
      "4681/4681 [==============================] - 226s 48ms/step - loss: 0.3500 - accuracy: 0.8543 - val_loss: 1.2034 - val_accuracy: 0.4731\n",
      "Epoch 9/10\n",
      "4681/4681 [==============================] - 225s 48ms/step - loss: 0.3058 - accuracy: 0.8654 - val_loss: 1.6081 - val_accuracy: 0.4791\n",
      "Epoch 10/10\n",
      "4681/4681 [==============================] - 225s 48ms/step - loss: 0.2373 - accuracy: 0.9056 - val_loss: 2.1764 - val_accuracy: 0.4372\n",
      "Accuracy score = 0.8091934084995663\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88       865\n",
      "           1       0.64      0.53      0.58       288\n",
      "\n",
      "    accuracy                           0.81      1153\n",
      "   macro avg       0.75      0.72      0.73      1153\n",
      "weighted avg       0.80      0.81      0.80      1153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################BiLSTM Attention without any dropouts ######################\n",
    "d2=0.0\n",
    "d1=0.0\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(len(word_index)+1, 300, input_length=129, weights=[embedding_vector], trainable=False))\n",
    "model_glove.add(Bidirectional(LSTM(300, return_sequences=True, dropout=d1,recurrent_dropout=d2)))\n",
    "model_glove.add(Attention(attention_activation='sigmoid'))\n",
    "model_glove.add(Flatten())\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit train data\n",
    "model_glove.fit(X_train, y_train, validation_split=0.2, epochs = 10)\n",
    "y_pred = model_glove.predict_classes(X_test)\n",
    "y_test=df_test[\"class\"].values.tolist()\n",
    "print(\"Accuracy score =\", accuracy_score(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "O_o_JNSMa4or",
    "outputId": "30bd58f5-2c84-48bb-ca8c-c63bfa8e0af7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4681 samples, validate on 1171 samples\n",
      "Epoch 1/6\n",
      "4681/4681 [==============================] - 229s 49ms/step - loss: 0.5881 - accuracy: 0.7125 - val_loss: 0.8163 - val_accuracy: 0.4910\n",
      "Epoch 2/6\n",
      "4681/4681 [==============================] - 228s 49ms/step - loss: 0.5437 - accuracy: 0.7349 - val_loss: 0.9642 - val_accuracy: 0.3860\n",
      "Epoch 3/6\n",
      "4681/4681 [==============================] - 232s 49ms/step - loss: 0.5263 - accuracy: 0.7424 - val_loss: 0.9707 - val_accuracy: 0.4039\n",
      "Epoch 4/6\n",
      "4681/4681 [==============================] - 232s 50ms/step - loss: 0.5143 - accuracy: 0.7535 - val_loss: 1.1224 - val_accuracy: 0.3775\n",
      "Epoch 5/6\n",
      "4681/4681 [==============================] - 229s 49ms/step - loss: 0.5039 - accuracy: 0.7571 - val_loss: 1.0750 - val_accuracy: 0.3424\n",
      "Epoch 6/6\n",
      "4681/4681 [==============================] - 233s 50ms/step - loss: 0.4827 - accuracy: 0.7695 - val_loss: 0.8940 - val_accuracy: 0.3834\n",
      "Accuracy score = 0.8230702515177797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89       865\n",
      "           1       0.74      0.44      0.56       288\n",
      "\n",
      "    accuracy                           0.82      1153\n",
      "   macro avg       0.79      0.70      0.72      1153\n",
      "weighted avg       0.81      0.82      0.81      1153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################BiLSTM Attention Dropouts-0.2,0.1,0.05######################\n",
    "d2=0.2\n",
    "d1=0.1\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(len(word_index)+1, 300, input_length=129, weights=[embedding_vector], trainable=False))\n",
    "model_glove.add(Bidirectional(LSTM(300, return_sequences=True, dropout=d1,recurrent_dropout=d2)))\n",
    "model_glove.add(Attention(attention_activation='sigmoid'))\n",
    "model_glove.add(Flatten())\n",
    "model_glove.add(Dense(10, activation='relu'))\n",
    "model_glove.add(Dropout(0.05)) \n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit train data\n",
    "model_glove.fit(X_train, y_train, validation_split=0.2, epochs = 6)\n",
    "y_pred = model_glove.predict_classes(X_test)\n",
    "y_test=df_test[\"class\"].values.tolist()\n",
    "print(\"Accuracy score =\", accuracy_score(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "S4Hsr7qrruyT",
    "outputId": "318c360a-34d2-4599-dede-84fdfd8910c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4681 samples, validate on 1171 samples\n",
      "Epoch 1/6\n",
      "4681/4681 [==============================] - 227s 48ms/step - loss: 0.5773 - accuracy: 0.7223 - val_loss: 0.9363 - val_accuracy: 0.3817\n",
      "Epoch 2/6\n",
      "4681/4681 [==============================] - 234s 50ms/step - loss: 0.5351 - accuracy: 0.7454 - val_loss: 0.9259 - val_accuracy: 0.3945\n",
      "Epoch 3/6\n",
      "4681/4681 [==============================] - 228s 49ms/step - loss: 0.5169 - accuracy: 0.7550 - val_loss: 1.1660 - val_accuracy: 0.3621\n",
      "Epoch 4/6\n",
      "4681/4681 [==============================] - 227s 48ms/step - loss: 0.5041 - accuracy: 0.7631 - val_loss: 1.0445 - val_accuracy: 0.4082\n",
      "Epoch 5/6\n",
      "4681/4681 [==============================] - 228s 49ms/step - loss: 0.4717 - accuracy: 0.7791 - val_loss: 1.0439 - val_accuracy: 0.3945\n",
      "Epoch 6/6\n",
      "4681/4681 [==============================] - 226s 48ms/step - loss: 0.4452 - accuracy: 0.7973 - val_loss: 1.0614 - val_accuracy: 0.4022\n",
      "Accuracy score = 0.8100607111882047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88       865\n",
      "           1       0.69      0.44      0.54       288\n",
      "\n",
      "    accuracy                           0.81      1153\n",
      "   macro avg       0.76      0.69      0.71      1153\n",
      "weighted avg       0.80      0.81      0.79      1153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################BiLSTM Attention Dropouts-0.2,0.1,0.05######################\n",
    "d2=0.0\n",
    "d1=0.0\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(len(word_index)+1, 300, input_length=129, weights=[embedding_vector], trainable=False))\n",
    "model_glove.add(Bidirectional(LSTM(300, return_sequences=True, dropout=d1,recurrent_dropout=d2)))\n",
    "model_glove.add(Attention(attention_activation='sigmoid'))\n",
    "model_glove.add(Flatten())\n",
    "#model_glove.add(Dense(10, activation='relu'))\n",
    "#model_glove.add(Dropout(0.05)) \n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit train data\n",
    "model_glove.fit(X_train, y_train, validation_split=0.2, epochs = 6)\n",
    "y_pred = model_glove.predict_classes(X_test)\n",
    "y_test=df_test[\"class\"].values.tolist()\n",
    "print(\"Accuracy score =\", accuracy_score(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dqHQ4CytobwH",
    "outputId": "cef493d4-dff5-4f3d-cfb4-0eafb361b42e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[808,  57],\n",
       "       [162, 126]])"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "UuCLPSWLHtv3",
    "outputId": "476a97b7-73b1-4104-c2c0-daae348e6ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4681 samples, validate on 1171 samples\n",
      "Epoch 1/5\n",
      "4681/4681 [==============================] - 231s 49ms/step - loss: 0.5899 - accuracy: 0.7020 - val_loss: 0.9900 - val_accuracy: 0.2921\n",
      "Epoch 2/5\n",
      "4681/4681 [==============================] - 231s 49ms/step - loss: 0.5516 - accuracy: 0.7272 - val_loss: 0.9772 - val_accuracy: 0.3348\n",
      "Epoch 3/5\n",
      "4681/4681 [==============================] - 230s 49ms/step - loss: 0.5423 - accuracy: 0.7278 - val_loss: 1.0722 - val_accuracy: 0.3091\n",
      "Epoch 4/5\n",
      "4681/4681 [==============================] - 232s 50ms/step - loss: 0.5313 - accuracy: 0.7432 - val_loss: 0.9881 - val_accuracy: 0.3365\n",
      "Epoch 5/5\n",
      "4681/4681 [==============================] - 231s 49ms/step - loss: 0.5206 - accuracy: 0.7398 - val_loss: 0.9451 - val_accuracy: 0.3766\n",
      "Accuracy score = 0.8074588031222897\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.97      0.88       865\n",
      "           1       0.77      0.33      0.46       288\n",
      "\n",
      "    accuracy                           0.81      1153\n",
      "   macro avg       0.79      0.65      0.67      1153\n",
      "weighted avg       0.80      0.81      0.78      1153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d2=0.2\n",
    "d1=0.25\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(len(word_index)+1, 300, input_length=129, weights=[embedding_vector], trainable=False))\n",
    "model_glove.add(Bidirectional(LSTM(300, return_sequences=True, dropout=d1,recurrent_dropout=d2)))\n",
    "model_glove.add(Attention(attention_activation='sigmoid'))\n",
    "model_glove.add(Flatten())\n",
    "model_glove.add(Dense(10, activation='relu'))\n",
    "model_glove.add(Dropout(0.1)) \n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit train data\n",
    "model_glove.fit(X_train, y_train, validation_split=0.2, epochs = 5)\n",
    "y_pred = model_glove.predict_classes(X_test)\n",
    "y_test=df_test[\"class\"].values.tolist()\n",
    "print(\"Accuracy score =\", accuracy_score(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "W1H22UCf6Ept",
    "outputId": "cec1da93-3c3f-4ec6-9e8e-8bb17f5ddd55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4681 samples, validate on 1171 samples\n",
      "Epoch 1/5\n",
      "4681/4681 [==============================] - 24s 5ms/step - loss: 0.5761 - accuracy: 0.7127 - val_loss: 0.8851 - val_accuracy: 0.3740\n",
      "Epoch 2/5\n",
      "4681/4681 [==============================] - 24s 5ms/step - loss: 0.5030 - accuracy: 0.7620 - val_loss: 1.1924 - val_accuracy: 0.3595\n",
      "Epoch 3/5\n",
      "4681/4681 [==============================] - 24s 5ms/step - loss: 0.4316 - accuracy: 0.8097 - val_loss: 1.0580 - val_accuracy: 0.4048\n",
      "Epoch 4/5\n",
      "4681/4681 [==============================] - 23s 5ms/step - loss: 0.3197 - accuracy: 0.8705 - val_loss: 0.9961 - val_accuracy: 0.5260\n",
      "Epoch 5/5\n",
      "4681/4681 [==============================] - 24s 5ms/step - loss: 0.2326 - accuracy: 0.9124 - val_loss: 1.5197 - val_accuracy: 0.4663\n",
      "Accuracy score = 0.779705117085863\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85       865\n",
      "           1       0.55      0.61      0.58       288\n",
      "\n",
      "    accuracy                           0.78      1153\n",
      "   macro avg       0.71      0.72      0.72      1153\n",
      "weighted avg       0.79      0.78      0.78      1153\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[724, 141],\n",
       "       [113, 175]])"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########LSTM+CNN-Modified features#########\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(len(word_index)+1, 300, input_length=129, weights=[embedding_vector], trainable=False))\n",
    "model_glove.add(Conv1D(32, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(300, return_sequences=True))#, dropout=0.05,recurrent_dropout=0.05))\n",
    "model_glove.add(Flatten())\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit train data\n",
    "y_train=df_train[\"class\"].values.tolist()\n",
    "model_glove.fit(X_train, y_train, validation_split=0.2, epochs = 5)\n",
    "y_pred = model_glove.predict_classes(X_test)\n",
    "y_test=df_test[\"class\"].values.tolist()\n",
    "print(\"Accuracy score =\", accuracy_score(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))      \n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "llXpP1ZBlxj2",
    "outputId": "15f1540d-1fed-4777-85c4-4cdee1f63dbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4681 samples, validate on 1171 samples\n",
      "Epoch 1/5\n",
      "4681/4681 [==============================] - 25s 5ms/step - loss: 0.5828 - accuracy: 0.7112 - val_loss: 0.8565 - val_accuracy: 0.3860\n",
      "Epoch 2/5\n",
      "4681/4681 [==============================] - 24s 5ms/step - loss: 0.5265 - accuracy: 0.7477 - val_loss: 0.9039 - val_accuracy: 0.4253\n",
      "Epoch 3/5\n",
      "4681/4681 [==============================] - 24s 5ms/step - loss: 0.4708 - accuracy: 0.7787 - val_loss: 0.8216 - val_accuracy: 0.4586\n",
      "Epoch 4/5\n",
      "4681/4681 [==============================] - 24s 5ms/step - loss: 0.4101 - accuracy: 0.8088 - val_loss: 1.0679 - val_accuracy: 0.4330\n",
      "Epoch 5/5\n",
      "4681/4681 [==============================] - 24s 5ms/step - loss: 0.3473 - accuracy: 0.8541 - val_loss: 1.0971 - val_accuracy: 0.4492\n",
      "Accuracy score = 0.7623590633130962\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.84       865\n",
      "           1       0.52      0.54      0.53       288\n",
      "\n",
      "    accuracy                           0.76      1153\n",
      "   macro avg       0.68      0.69      0.69      1153\n",
      "weighted avg       0.77      0.76      0.76      1153\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[723, 142],\n",
       "       [132, 156]])"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########LSTM+CNN-Modified features#########\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(len(word_index)+1, 300, input_length=129, weights=[embedding_vector], trainable=False))\n",
    "model_glove.add(Conv1D(32, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(300, return_sequences=True, dropout=0.2,recurrent_dropout=0.1))\n",
    "model_glove.add(Flatten())\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit train data\n",
    "y_train=df_train[\"class\"].values.tolist()\n",
    "model_glove.fit(X_train, y_train, validation_split=0.2, epochs = 5)\n",
    "y_pred = model_glove.predict_classes(X_test)\n",
    "y_test=df_test[\"class\"].values.tolist()\n",
    "print(\"Accuracy score =\", accuracy_score(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))      \n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "WZTL06joQcRf",
    "outputId": "3a5a0fee-d972-4f25-b4df-90b959968286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4681 samples, validate on 1171 samples\n",
      "Epoch 1/7\n",
      "4681/4681 [==============================] - 227s 48ms/step - loss: 0.5802 - accuracy: 0.7131 - val_loss: 1.0040 - val_accuracy: 0.3279\n",
      "Epoch 2/7\n",
      "4681/4681 [==============================] - 224s 48ms/step - loss: 0.5375 - accuracy: 0.7347 - val_loss: 1.0232 - val_accuracy: 0.3587\n",
      "Epoch 3/7\n",
      "4681/4681 [==============================] - 228s 49ms/step - loss: 0.5145 - accuracy: 0.7477 - val_loss: 0.9048 - val_accuracy: 0.3416\n",
      "Epoch 4/7\n",
      "4681/4681 [==============================] - 223s 48ms/step - loss: 0.5021 - accuracy: 0.7539 - val_loss: 0.9867 - val_accuracy: 0.3757\n",
      "Epoch 5/7\n",
      "4681/4681 [==============================] - 225s 48ms/step - loss: 0.4772 - accuracy: 0.7742 - val_loss: 1.2079 - val_accuracy: 0.3621\n",
      "Epoch 6/7\n",
      "4681/4681 [==============================] - 227s 48ms/step - loss: 0.4591 - accuracy: 0.7795 - val_loss: 1.0818 - val_accuracy: 0.4278\n",
      "Epoch 7/7\n",
      "4681/4681 [==============================] - 225s 48ms/step - loss: 0.4299 - accuracy: 0.7979 - val_loss: 0.9666 - val_accuracy: 0.4270\n",
      "Accuracy score = 0.813529921942758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88       865\n",
      "           1       0.67      0.51      0.58       288\n",
      "\n",
      "    accuracy                           0.81      1153\n",
      "   macro avg       0.76      0.71      0.73      1153\n",
      "weighted avg       0.80      0.81      0.80      1153\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[792,  73],\n",
       "       [142, 146]])"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################BiLSTM Attention+ Relu without any dropouts ######################\n",
    "d2=0.0\n",
    "d1=0.0\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(len(word_index)+1, 300, input_length=129, weights=[embedding_vector], trainable=False))\n",
    "model_glove.add(Bidirectional(LSTM(300, return_sequences=True, dropout=d1,recurrent_dropout=d2)))\n",
    "model_glove.add(Attention(attention_activation='sigmoid'))\n",
    "model_glove.add(Flatten())\n",
    "model_glove.add(Dense(10, activation='relu'))\n",
    "model_glove.add(Dropout(0.0)) \n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit train data\n",
    "model_glove.fit(X_train, y_train, validation_split=0.2, epochs = 7)\n",
    "y_pred = model_glove.predict_classes(X_test)\n",
    "y_test=df_test[\"class\"].values.tolist()\n",
    "print(\"Accuracy score =\", accuracy_score(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))   \n",
    "confusion_matrix(y_test, y_pred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "6Pgq7Zl1VvnW",
    "outputId": "9d3c78ec-9a6e-4998-b0dd-2604ffdc4cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4681 samples, validate on 1171 samples\n",
      "Epoch 1/10\n",
      "4681/4681 [==============================] - 230s 49ms/step - loss: 0.5960 - accuracy: 0.6981 - val_loss: 0.9824 - val_accuracy: 0.3493\n",
      "Epoch 2/10\n",
      "4681/4681 [==============================] - 230s 49ms/step - loss: 0.5566 - accuracy: 0.7052 - val_loss: 1.0724 - val_accuracy: 0.2750\n",
      "Epoch 3/10\n",
      "4681/4681 [==============================] - 229s 49ms/step - loss: 0.5398 - accuracy: 0.6984 - val_loss: 0.9643 - val_accuracy: 0.2750\n",
      "Epoch 4/10\n",
      "4681/4681 [==============================] - 233s 50ms/step - loss: 0.5330 - accuracy: 0.7287 - val_loss: 0.9721 - val_accuracy: 0.4620\n",
      "Epoch 5/10\n",
      "4681/4681 [==============================] - 231s 49ms/step - loss: 0.5103 - accuracy: 0.7537 - val_loss: 0.8975 - val_accuracy: 0.4330\n",
      "Epoch 6/10\n",
      "4681/4681 [==============================] - 230s 49ms/step - loss: 0.4850 - accuracy: 0.7695 - val_loss: 1.0904 - val_accuracy: 0.4535\n",
      "Epoch 7/10\n",
      "4681/4681 [==============================] - 232s 50ms/step - loss: 0.4650 - accuracy: 0.7840 - val_loss: 0.9808 - val_accuracy: 0.4833\n",
      "Epoch 8/10\n",
      "4681/4681 [==============================] - 230s 49ms/step - loss: 0.4332 - accuracy: 0.8007 - val_loss: 1.0952 - val_accuracy: 0.4372\n",
      "Epoch 9/10\n",
      "4681/4681 [==============================] - 231s 49ms/step - loss: 0.4066 - accuracy: 0.8169 - val_loss: 1.0710 - val_accuracy: 0.4441\n",
      "Epoch 10/10\n",
      "4681/4681 [==============================] - 236s 50ms/step - loss: 0.3712 - accuracy: 0.8374 - val_loss: 1.4450 - val_accuracy: 0.4424\n",
      "Accuracy score = 0.7987857762359063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86       865\n",
      "           1       0.59      0.62      0.61       288\n",
      "\n",
      "    accuracy                           0.80      1153\n",
      "   macro avg       0.73      0.74      0.74      1153\n",
      "weighted avg       0.80      0.80      0.80      1153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################BiLSTM Attention Dropouts-0.2,0.05,0.05######################\n",
    "d2=0.2\n",
    "d1=0.05\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(len(word_index)+1, 300, input_length=129, weights=[embedding_vector], trainable=False))\n",
    "model_glove.add(Bidirectional(LSTM(300, return_sequences=True, dropout=d1,recurrent_dropout=d2)))\n",
    "model_glove.add(Attention(attention_activation='sigmoid'))\n",
    "model_glove.add(Flatten())\n",
    "model_glove.add(Dense(10, activation='relu'))\n",
    "model_glove.add(Dropout(0.05)) \n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit train data\n",
    "model_glove.fit(X_train, y_train, validation_split=0.2, epochs = 10)\n",
    "y_pred = model_glove.predict_classes(X_test)\n",
    "y_test=df_test[\"class\"].values.tolist()\n",
    "print(\"Accuracy score =\", accuracy_score(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tu-ukRx_Vyaw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Experiment3_Text_BiLSTM_Attention_GLove_git.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
